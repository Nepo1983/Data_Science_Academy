{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d835750b",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Matemática Para Data Science</font>\n",
    "\n",
    "## <font color='blue'>Estudo de Caso 8</font>\n",
    "### <font color='blue'>Implementando o Algoritmo Backpropagation em Linguagem Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e2305",
   "metadata": {},
   "source": [
    "![DSA](imagens/EC8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb8f2f",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3980f02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.18\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "# !pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eff698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab885c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "numpy: 1.26.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1d06e",
   "metadata": {},
   "source": [
    "## Algoritmo Backpropagation\n",
    "\n",
    "O algoritmo Backpropagation, ou retropropagação, é um método amplamente utilizado no treinamento de redes neurais. É um algoritmo de otimização baseado em gradientes que minimiza a função de custo, ajustando iterativamente os pesos da rede. O Backpropagation é utilizado em conjunto com um algoritmo de otimização, como o Gradiente Descendente, para realizar essa tarefa.\n",
    "\n",
    "Aqui está como o algoritmo Backpropagation funciona, dividido em etapas:\n",
    "\n",
    "**1- Feedforward:**\n",
    "\n",
    "- Começa com a entrada através da rede, camada por camada, até chegar à saída.\n",
    "- Utiliza os pesos atuais da rede para calcular a saída para cada neurônio.\n",
    "\n",
    "**2- Cálculo do Erro:**\n",
    "\n",
    "- Compara a saída calculada da rede com a saída desejada (rótulo verdadeiro).\n",
    "- Utiliza uma função de custo (como a entropia cruzada ou erro quadrático médio) para calcular o erro total da rede.\n",
    "\n",
    "**3- Retropropagação do Erro:**\n",
    "\n",
    "- Calcula o gradiente da função de custo em relação a cada peso, movendo-se da camada de saída para a camada de entrada.\n",
    "- Utiliza a regra da cadeia para calcular esses gradientes, o que envolve calcular derivadas parciais de várias quantidades, como a função de ativação e a função de custo.\n",
    "\n",
    "A propagação do gradiente através das camadas é o que dá o nome de \"retropropagação\".\n",
    "\n",
    "**4- Atualização dos Pesos:**\n",
    "\n",
    "- Utiliza os gradientes calculados junto com um algoritmo de otimização (como o Gradiente Descendente) para ajustar os pesos na direção que reduz o erro.\n",
    "- O tamanho da mudança em cada peso é determinado pela taxa de aprendizado, que é um hiperparâmetro escolhido manualmente.\n",
    "\n",
    "**5- Iteração:**\n",
    "\n",
    "- Repete as etapas de 1 a 4 para várias épocas ou até que o erro da rede atinja um valor aceitável.\n",
    "\n",
    "O algoritmo Backpropagation foi uma inovação crítica que permitiu o treinamento eficiente de redes neurais profundas e é uma parte central da maioria dos frameworks modernos de aprendizado profundo. Ao ajustar os pesos da rede de maneira que minimizem a função de custo, ele permite que a rede aprenda representações complexas dos dados de entrada e faça previsões precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4aecd",
   "metadata": {},
   "source": [
    "## Pseudo-Código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0f52b",
   "metadata": {},
   "source": [
    "![DSA](imagens/pseudo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee3959",
   "metadata": {},
   "source": [
    "O algoritmo Backpropagation é um método iterativo para treinar redes neurais. Abaixo está o pseudo-código para o algoritmo:\n",
    "\n",
    "Inicialize os pesos da rede:\n",
    "\n",
    "Inicialize os pesos da rede com pequenos valores aleatórios.\n",
    "Para cada época até a convergência, faça:\n",
    "\n",
    "a. Para cada exemplo de treinamento na base de dados, faça:\n",
    "\n",
    "i. Feedforward:\n",
    "\n",
    "- Entrada: X (exemplo de entrada).\n",
    "- Para cada camada da rede, calcule a saída de cada neurônio com os pesos e a função de ativação.\n",
    "- Guarde as ativações e as saídas de cada camada.\n",
    "\n",
    "ii. Calcule o erro da camada de saída:\n",
    "\n",
    "- Calcule o erro da saída da rede comparando com o rótulo verdadeiro Y.\n",
    "- Utilize uma função de custo, como erro quadrático médio.\n",
    "\n",
    "iii. Retropropagação do Erro:\n",
    "\n",
    "- Para cada camada, começando pela camada de saída e movendo-se para trás:\n",
    "- Calcule o gradiente do erro em relação às ativações da camada.\n",
    "- Calcule o gradiente do erro em relação aos pesos da camada, usando a regra da cadeia.\n",
    "- Guarde os gradientes de cada camada.\n",
    "\n",
    "iv. Atualização dos Pesos:\n",
    "\n",
    "- Para cada camada:\n",
    "- Atualize os pesos da camada usando o gradiente calculado e a taxa de aprendizado.\n",
    "- Os pesos podem ser atualizados usando métodos como Gradiente Descendente.\n",
    "\n",
    "b. Verifique a Convergência:\n",
    "\n",
    "Avalie a rede no conjunto de validação, se disponível.\n",
    "Se o erro no conjunto de validação estiver abaixo de um limiar, ou se o erro parar de diminuir, pare o treinamento.\n",
    "Retorne os pesos treinados da rede.\n",
    "\n",
    "O pseudo-código acima descreve o treinamento de uma rede neural usando Backpropagation. É um resumo de alto nível, e a implementação real pode exigir detalhes adicionais, como escolha da função de ativação, inicialização de peso, técnica de otimização, etc. A eficácia do treinamento pode ser sensível à escolha desses detalhes e hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62cacc",
   "metadata": {},
   "source": [
    "## Implementação em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74eb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada da função sigmóide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36755b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de neurônios em cada camada da rede\n",
    "input_neurons, hidden_neurons, output_neurons = 2, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b292693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização de pesos e bias da camada de entrada para a camada oculta\n",
    "weights_input_hidden = np.random.uniform(size = (input_neurons, hidden_neurons))\n",
    "bias_input_hidden = np.random.uniform(size = (1, hidden_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e772a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização de pesos e bias da camada oculta para a camada de saída\n",
    "weights_hidden_output = np.random.uniform(size = (hidden_neurons, output_neurons))\n",
    "bias_hidden_output = np.random.uniform(size = (1, output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6755976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "X = np.array([[0, 0],[0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output esperado\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73aff462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "lr = 0.7\n",
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea396a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treino\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Feedforward\n",
    "    # Calcula a ativação da camada oculta (hidden layer) usando os pesos de entrada\n",
    "    hidden_layer_activation = np.dot(X, weights_input_hidden)\n",
    "    \n",
    "    # Adiciona o bias à ativação da camada oculta\n",
    "    hidden_layer_activation += bias_input_hidden\n",
    "    \n",
    "    # Aplica a função sigmoid para calcular a saída da camada oculta\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    # Calcula a ativação da camada de saída usando os pesos entre a camada oculta e a camada de saída\n",
    "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    \n",
    "    # Adiciona o bias à ativação da camada de saída\n",
    "    output_layer_activation += bias_hidden_output\n",
    "    \n",
    "    # Aplica a função sigmoid para calcular a saída prevista\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Calcula o erro entre a saída prevista e a saída verdadeira\n",
    "    error = y - predicted_output\n",
    "    \n",
    "    # Calcula a derivada do erro em relação à saída prevista, usando a derivada da função sigmoid\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    # Calcula o erro da camada oculta, propagando o erro da camada de saída\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "        \n",
    "    # Calcula a derivada do erro em relação à saída da camada oculta\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Atualizando os pesos e os bias\n",
    "    # Atualiza os pesos da camada oculta para a camada de saída usando a taxa de aprendizado\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    \n",
    "    # Atualiza o bias da camada de saída usando a taxa de aprendizado\n",
    "    bias_hidden_output += np.sum(d_predicted_output, axis = 0, keepdims = True) * lr\n",
    "    \n",
    "    # Atualiza os pesos da camada de entrada para a camada oculta usando a taxa de aprendizado\n",
    "    weights_input_hidden += X.T.dot(d_hidden_layer) * lr\n",
    "    \n",
    "    # Atualiza o bias da camada oculta usando a taxa de aprendizado\n",
    "    bias_input_hidden += np.sum(d_hidden_layer, axis = 0, keepdims = True) * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf5e68ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0158039 ]\n",
      " [0.98643808]\n",
      " [0.98643507]\n",
      " [0.01399347]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7cb0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576ca226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.70812482 6.59053076]\n",
      " [4.70895404 6.59391294]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5508f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.66308156]\n",
      " [  9.95714187]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42c1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.22511647 -2.94765901]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c3b1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.62020316]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e6c8f",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
